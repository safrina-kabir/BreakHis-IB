{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26053a9a",
   "metadata": {},
   "source": [
    "\n",
    "# BreakHis-IB Split Builder üìöüß™\n",
    "Create **patient-held-out**, **subtype-balanced per magnification** evaluation splits for the **BreakHis** dataset, plus a natural-frequency comparator ‚Äî without redistributing images.\n",
    "\n",
    "**What this notebook does**\n",
    "- Scans your local BreakHis root to build a metadata table (path, patient_id, subtype, benign/malignant, magnification).\n",
    "- Builds two evaluation protocols:\n",
    "  1. **StrictBalancedTest**: equal counts across **8 subtypes √ó 4 magnifications** (subject to data availability), patient-held-out.\n",
    "  2. **NaturalTest**: patient-held-out, mirrors the original distribution.\n",
    "- Creates **Validation** (balanced, smaller) and **Train** from the remaining patients.\n",
    "- Saves split CSVs and a **leakage check** report.\n",
    "- (Optional) **Materialize** splits as **symlinks** or **copies** to a new folder.\n",
    "\n",
    "> ‚ö†Ô∏è **Licensing:** This notebook does **not** rehost BreakHis images. It only creates split metadata (CSVs) and optionally symlinks/copies from your local copy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98586f9d",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Configuration\n",
    "Set your local paths and split preferences here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8778d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ==== REQUIRED: Set the path where you've extracted BreakHis ====\n",
    "# Example structure (may vary):\n",
    "#   /data/BreaKHis_v1/\n",
    "#     ‚îî‚îÄ‚îÄ histology_slides/breast/\n",
    "#           ‚îú‚îÄ‚îÄ benign/adenosis/.../*.png\n",
    "#           ‚îú‚îÄ‚îÄ benign/fibroadenoma/.../*.png\n",
    "#           ‚îú‚îÄ‚îÄ malignant/ductal_carcinoma/.../*.png\n",
    "#           ‚îî‚îÄ‚îÄ ...\n",
    "BREAKHIS_ROOT = Path(\"/path/to/BreaKHis_v1\")   # <-- CHANGE THIS\n",
    "\n",
    "# ==== Where to save outputs ====\n",
    "OUT_DIR = Path(\"./breakhis_ib_outputs\")         # CSVs & reports\n",
    "MATERIALIZE_IMAGES = False                      # True to create split folders with symlinks/copies\n",
    "MATERIALIZE_METHOD = \"symlink\"                  # \"symlink\" or \"copy\"\n",
    "MATERIALIZE_DIR = Path(\"./breakhis_ib_materialized\")  # target dir if MATERIALIZE_IMAGES=True\n",
    "\n",
    "# ==== Split behavior ====\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Strict balanced test target per (magnification, subtype).\n",
    "# Use None to auto-compute the feasible K (minimum available across subtypes per magnification).\n",
    "STRICT_TEST_K = None     # e.g., set to 30 if you want a fixed target (will adapt downward if impossible)\n",
    "\n",
    "# Validation target per cell (magnification, subtype) for strict-balanced protocol.\n",
    "VALIDATION_K = 10        # keep modest to avoid starving the train set\n",
    "\n",
    "# Natural test fraction per patient (approximate). You can also set ABS_N_TEST_IMAGES instead.\n",
    "NATURAL_TEST_FRACTION = 0.2\n",
    "ABS_N_TEST_IMAGES = None  # If set (e.g., 1200), overrides NATURAL_TEST_FRACTION\n",
    "\n",
    "# File patterns to include (feel free to add extensions)\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\"}\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if MATERIALIZE_IMAGES:\n",
    "    MATERIALIZE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configured. Root:\", BREAKHIS_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9643f570",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports & helper tables\n",
    "We define subtype mappings, robust filename/path parsers, and some utility functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f631929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Subtype canonical names & short codes used in BreakHis papers/directory names.\n",
    "SUBTYPE_CANON = {\n",
    "    # Benign\n",
    "    \"adenosis\": (\"A\", \"benign\"),\n",
    "    \"fibroadenoma\": (\"F\", \"benign\"),\n",
    "    \"phyllodes_tumor\": (\"PT\", \"benign\"),\n",
    "    \"tubular_adenoma\": (\"TA\", \"benign\"),\n",
    "    # Malignant\n",
    "    \"ductal_carcinoma\": (\"DC\", \"malignant\"),\n",
    "    \"lobular_carcinoma\": (\"LC\", \"malignant\"),\n",
    "    \"mucinous_carcinoma\": (\"MC\", \"malignant\"),\n",
    "    \"papillary_carcinoma\": (\"PC\", \"malignant\"),\n",
    "}\n",
    "\n",
    "# Alternate names we might see in paths\n",
    "SUBTYPE_ALIASES = {\n",
    "    \"adenosis\": \"adenosis\",\n",
    "    \"fibroadenoma\": \"fibroadenoma\",\n",
    "    \"phyllodes\": \"phyllodes_tumor\",\n",
    "    \"phyllodes_tumor\": \"phyllodes_tumor\",\n",
    "    \"tubular_adenoma\": \"tubular_adenoma\",\n",
    "    \"ductal_carcinoma\": \"ductal_carcinoma\",\n",
    "    \"ductal\": \"ductal_carcinoma\",\n",
    "    \"lobular_carcinoma\": \"lobular_carcinoma\",\n",
    "    \"lobular\": \"lobular_carcinoma\",\n",
    "    \"mucinous_carcinoma\": \"mucinous_carcinoma\",\n",
    "    \"mucinous\": \"mucinous_carcinoma\",\n",
    "    \"papillary_carcinoma\": \"papillary_carcinoma\",\n",
    "    \"papillary\": \"papillary_carcinoma\",\n",
    "}\n",
    "\n",
    "SUBTYPE_CODE2NAME = {v[0]: k for k, v in SUBTYPE_CANON.items()}\n",
    "\n",
    "MAG_REGEX = re.compile(r\"(?:^|[_\\-\\/])(?P<mag>40|100|200|400)X(?:[_\\-\\/]|$)\", re.IGNORECASE)\n",
    "\n",
    "# Example filename pattern seen in BreakHis tiles, e.g.:\n",
    "# SOB_B_A-14-22549-40X-001.png  or  SOB_M_DC-14-98765-200X-003.png\n",
    "FNAME_REGEX = re.compile(\n",
    "    r\"SOB[_-](?P<bm>[BM])[_-](?P<code>A|F|PT|TA|DC|LC|MC|PC)-(?P<pnum>\\d+)-\\d+-?(?P<mag>40|100|200|400)X\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def find_in_path(path_str, words):\n",
    "    low = path_str.lower()\n",
    "    for w in words:\n",
    "        if w in low:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def infer_label_from_path(p: Path):\n",
    "    s = p.as_posix().lower()\n",
    "    if \"benign\" in s:\n",
    "        return \"benign\"\n",
    "    if \"malignant\" in s:\n",
    "        return \"malignant\"\n",
    "    return None\n",
    "\n",
    "def infer_subtype_from_path(p: Path):\n",
    "    s = p.as_posix().lower()\n",
    "    # check canonical & alias keys\n",
    "    for alias, canon in SUBTYPE_ALIASES.items():\n",
    "        if f\"/{alias}/\" in s or s.endswith(f\"/{alias}\") or f\"_{alias}_\" in s or f\"-{alias}-\" in s:\n",
    "            return canon\n",
    "    return None\n",
    "\n",
    "def infer_magnification(path: Path):\n",
    "    m = MAG_REGEX.search(path.as_posix())\n",
    "    if m:\n",
    "        return int(m.group(\"mag\"))\n",
    "    # try filename tokens like '40x'\n",
    "    base = path.name.lower()\n",
    "    for k in [40, 100, 200, 400]:\n",
    "        if f\"{k}x\" in base:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "def infer_from_filename(path: Path):\n",
    "    \"\"\"Parse from filename (SOB_B_A-14-xxxxx-40X-yy.png). Returns dict or {}.\"\"\"\n",
    "    m = FNAME_REGEX.search(path.name)\n",
    "    if not m:\n",
    "        # Try full path\n",
    "        m = FNAME_REGEX.search(path.as_posix())\n",
    "    if not m:\n",
    "        return {}\n",
    "    bm = m.group(\"bm\").upper()\n",
    "    label = \"benign\" if bm == \"B\" else \"malignant\"\n",
    "    code = m.group(\"code\").upper()\n",
    "    mag = int(m.group(\"mag\"))\n",
    "    pnum = m.group(\"pnum\")\n",
    "    # Patient id convention: <code>-<pnum>, e.g., A-14, DC-33\n",
    "    patient_id = f\"{code}-{pnum}\"\n",
    "    subtype = SUBTYPE_CODE2NAME.get(code)\n",
    "    return {\"label\": label, \"subtype\": subtype, \"subtype_code\": code, \"magnification\": mag, \"patient_id\": patient_id}\n",
    "\n",
    "def robust_parse_record(path: Path):\n",
    "    \"\"\"Return dict with keys: path, label, subtype, subtype_code, magnification, patient_id.\n",
    "    Tries directory cues first, then filename regex. Records 'parse_issue' if anything missing.\n",
    "    \"\"\"\n",
    "    rec = {\"path\": str(path), \"label\": None, \"subtype\": None, \"subtype_code\": None, \"magnification\": None, \"patient_id\": None, \"parse_issue\": None}\n",
    "    # First, directory cues\n",
    "    label = infer_label_from_path(path)\n",
    "    subtype = infer_subtype_from_path(path)\n",
    "    magnification = infer_magnification(path)\n",
    "\n",
    "    if subtype in SUBTYPE_CANON:\n",
    "        code, canon_label = SUBTYPE_CANON[subtype]\n",
    "        if label is None:\n",
    "            label = canon_label\n",
    "        subtype_code = code\n",
    "    else:\n",
    "        subtype_code = None\n",
    "\n",
    "    # Attempt filename parse for anything missing (or patient_id always via filename when possible)\n",
    "    fname_guess = infer_from_filename(path)\n",
    "    for k in [\"label\", \"subtype\", \"subtype_code\", \"magnification\", \"patient_id\"]:\n",
    "        if (locals().get(k) is None) and (k in fname_guess and fname_guess[k] is not None):\n",
    "            locals()[k] = fname_guess[k]\n",
    "\n",
    "    # After merges, drop into rec\n",
    "    rec.update({\n",
    "        \"label\": label,\n",
    "        \"subtype\": subtype if subtype else fname_guess.get(\"subtype\"),\n",
    "        \"subtype_code\": subtype_code if subtype_code else fname_guess.get(\"subtype_code\"),\n",
    "        \"magnification\": magnification if magnification is not None else fname_guess.get(\"magnification\"),\n",
    "        \"patient_id\": fname_guess.get(\"patient_id\"),\n",
    "    })\n",
    "\n",
    "    # If any of the critical fields are missing, mark issue\n",
    "    required = [\"label\", \"subtype\", \"subtype_code\", \"magnification\", \"patient_id\"]\n",
    "    missing = [k for k in required if rec[k] in (None, \"\", \"unknown\")]\n",
    "    if missing:\n",
    "        rec[\"parse_issue\"] = f\"missing:{','.join(missing)}\"\n",
    "    return rec\n",
    "\n",
    "def ensure_dir(path: Path):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    import random, numpy as np\n",
    "    random.seed(seed); np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279778d",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Scan the BreakHis filesystem and build metadata\n",
    "This walks your `BREAKHIS_ROOT` and parses labels, subtypes, magnifications, and patient IDs.  \n",
    "Any files that cannot be parsed are logged to `parse_errors.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed59d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "if not BREAKHIS_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Set BREAKHIS_ROOT correctly. Not found: {BREAKHIS_ROOT}\")\n",
    "\n",
    "all_paths = []\n",
    "for p in BREAKHIS_ROOT.rglob(\"*\"):\n",
    "    if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "        all_paths.append(p)\n",
    "\n",
    "print(f\"Found {len(all_paths)} image files.\")\n",
    "\n",
    "recs = []\n",
    "for p in all_paths:\n",
    "    rec = robust_parse_record(p)\n",
    "    recs.append(rec)\n",
    "\n",
    "df = pd.DataFrame(recs)\n",
    "print(\"Parsed rows:\", len(df))\n",
    "\n",
    "# Save raw metadata\n",
    "meta_csv = OUT_DIR / \"metadata_raw.csv\"\n",
    "df.to_csv(meta_csv, index=False)\n",
    "print(\"Saved:\", meta_csv)\n",
    "\n",
    "# Keep only well-parsed rows\n",
    "ok = df[df[\"parse_issue\"].isna()].copy()\n",
    "bad = df[~df[\"parse_issue\"].isna()].copy()\n",
    "\n",
    "ok_csv = OUT_DIR / \"metadata_ok.csv\"\n",
    "bad_csv = OUT_DIR / \"parse_errors.csv\"\n",
    "ok.to_csv(ok_csv, index=False)\n",
    "bad.to_csv(bad_csv, index=False)\n",
    "print(f\"OK rows: {len(ok)}  | Parse issues: {len(bad)}\")\n",
    "print(\"Saved:\", ok_csv, \"and\", bad_csv)\n",
    "\n",
    "display(ok.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603eed1",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Sanity checks\n",
    "Quick counts by label, subtype, magnification, and patient summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def counts_table(data):\n",
    "    c1 = data.groupby([\"label\"]).size().rename(\"count\").reset_index()\n",
    "    c2 = data.groupby([\"subtype\"]).size().rename(\"count\").reset_index().sort_values(\"subtype\")\n",
    "    c3 = data.groupby([\"magnification\"]).size().rename(\"count\").reset_index().sort_values(\"magnification\")\n",
    "    c4 = data.groupby([\"magnification\",\"subtype\"]).size().rename(\"count\").reset_index()\n",
    "    n_patients = data[\"patient_id\"].nunique()\n",
    "    print(\"Unique patients:\", n_patients)\n",
    "    return c1, c2, c3, c4\n",
    "\n",
    "c1, c2, c3, c4 = counts_table(ok)\n",
    "display(c1); display(c2); display(c3); display(c4.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2908b6",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Split builders\n",
    "We create:\n",
    "- **StrictBalancedTest**: equal per `(magnification, subtype)` where feasible (auto-adjusts `K` downward if needed).\n",
    "- **Validation**: smaller balanced set from remaining patients.\n",
    "- **Train**: everything else.\n",
    "- **NaturalTest**: patient-held-out, approximates the natural class distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def sample_balanced_by_cell(df_pool: pd.DataFrame, target_k: Dict[Tuple[int,str], int], rng: np.random.RandomState):\n",
    "    \"\"\"Attempt to sample exactly target_k[(mag, subtype)] images per cell.\n",
    "    - Ensures no patient appears more than once *within* this sampled set? (Allowed; we ensure exclusivity across splits separately.)\n",
    "    - Returns a DataFrame and a dict of shortfalls for cells that couldn't meet K.\n",
    "    \"\"\"\n",
    "    picked = []\n",
    "    shortfalls = {}\n",
    "    # Shuffle order to avoid systematic biases; prioritize rare cells (lower available count)\n",
    "    avail = df_pool.groupby([\"magnification\",\"subtype\"]).size().rename(\"n\").reset_index()\n",
    "    order = avail.sort_values(\"n\").reset_index(drop=True)\n",
    "\n",
    "    for _, row in order.iterrows():\n",
    "        mag, st = int(row.magnification), row.subtype\n",
    "        K = target_k.get((mag, st), 0)\n",
    "        if K <= 0:\n",
    "            continue\n",
    "        cell = df_pool[(df_pool.magnification==mag) & (df_pool.subtype==st)]\n",
    "        # Prefer sampling across multiple patients: group by patient then round-robin\n",
    "        by_pid = {pid: g for pid, g in cell.groupby(\"patient_id\")}\n",
    "        if not by_pid:\n",
    "            shortfalls[(mag,st)] = K\n",
    "            continue\n",
    "        # Build a round-robin list of indices\n",
    "        rr = []\n",
    "        for pid, g in by_pid.items():\n",
    "            idxs = list(g.index)\n",
    "            rng.shuffle(idxs)\n",
    "            rr.append(idxs)\n",
    "        # Interleave to avoid taking too many from one patient\n",
    "        flat = []\n",
    "        while any(rr) and len(flat) < K:\n",
    "            for lst in rr:\n",
    "                if lst and len(flat) < K:\n",
    "                    flat.append(lst.pop())\n",
    "        if len(flat) < K:\n",
    "            shortfalls[(mag,st)] = K - len(flat)\n",
    "        picked.append(df_pool.loc[flat])\n",
    "    if picked:\n",
    "        out = pd.concat(picked).drop_duplicates()\n",
    "    else:\n",
    "        out = pd.DataFrame(columns=df_pool.columns)\n",
    "    return out, shortfalls\n",
    "\n",
    "def compute_auto_K(df_pool: pd.DataFrame) -> Dict[Tuple[int,str], int]:\n",
    "    \"\"\"For each magnification, set K to the minimum available across subtypes at that magnification.\"\"\"\n",
    "    target_k = {}\n",
    "    for mag, gmag in df_pool.groupby(\"magnification\"):\n",
    "        counts = gmag.groupby(\"subtype\").size()\n",
    "        # Require that each subtype exists; if a subtype is missing entirely at this magnification, K=0 for that cell.\n",
    "        min_k = int(counts.min()) if len(counts)>0 else 0\n",
    "        for st in SUBTYPE_CANON.keys():\n",
    "            # Only set K where subtype exists in pool\n",
    "            n = int(gmag[gmag.subtype==st].shape[0])\n",
    "            target_k[(int(mag), st)] = min(min_k, n)\n",
    "    return target_k\n",
    "\n",
    "def assign_split_strict_balanced(df_all: pd.DataFrame, strict_test_k=None, validation_k=10, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    pool = df_all.copy()\n",
    "\n",
    "    # Determine K for test\n",
    "    if strict_test_k is None:\n",
    "        target_k = compute_auto_K(pool)\n",
    "    else:\n",
    "        # Use fixed K but cap by availability per cell\n",
    "        target_k = {}\n",
    "        for mag, st in [(m,s) for m in sorted(pool.magnification.unique()) for s in SUBTYPE_CANON.keys()]:\n",
    "            n_avail = pool[(pool.magnification==mag) & (pool.subtype==st)].shape[0]\n",
    "            target_k[(int(mag), st)] = min(int(strict_test_k), int(n_avail))\n",
    "\n",
    "    # Sample StrictBalancedTest\n",
    "    test_bal, short1 = sample_balanced_by_cell(pool, target_k, rng)\n",
    "    # Adapt K downward if some cells shorted (common with rare subtypes)\n",
    "    if short1:\n",
    "        # Reduce K to feasible minimum per (mag) where needed\n",
    "        # Compute achievable K per cell\n",
    "        feas_k = {}\n",
    "        for (mag, st), want in target_k.items():\n",
    "            n = pool[(pool.magnification==mag) & (pool.subtype==st)].shape[0]\n",
    "            feas_k[(mag, st)] = min(want, n)\n",
    "        # Set common K per magnification to the min across subtypes\n",
    "        new_target = {}\n",
    "        for mag in sorted(pool.magnification.unique()):\n",
    "            ks = [feas_k[(mag, st)] for st in SUBTYPE_CANON.keys() if (mag, st) in feas_k]\n",
    "            if ks:\n",
    "                Kmag = min(ks)\n",
    "                for st in SUBTYPE_CANON.keys():\n",
    "                    new_target[(mag, st)] = Kmag\n",
    "        test_bal, short2 = sample_balanced_by_cell(pool, new_target, rng)\n",
    "        target_k = new_target  # update\n",
    "\n",
    "    # Mark test patients (held-out)\n",
    "    test_pids = set(test_bal.patient_id.unique())\n",
    "    remain = pool[~pool.patient_id.isin(test_pids)].copy()\n",
    "\n",
    "    # Build Validation (balanced, smaller)\n",
    "    val_target = {}\n",
    "    for (mag, st), K in target_k.items():\n",
    "        val_target[(mag, st)] = min(int(validation_k), int(remain[(remain.magnification==mag)&(remain.subtype==st)].shape[0]))\n",
    "\n",
    "    val_bal, shortv = sample_balanced_by_cell(remain, val_target, rng)\n",
    "    val_pids = set(val_bal.patient_id.unique())\n",
    "    remain2 = remain[~remain.patient_id.isin(val_pids)].copy()\n",
    "\n",
    "    train = remain2.copy()\n",
    "\n",
    "    # Compose split labels\n",
    "    df_splits = df_all.copy()\n",
    "    df_splits[\"split\"] = \"train\"\n",
    "    df_splits.loc[df_splits.index.isin(val_bal.index), \"split\"] = \"val\"\n",
    "    df_splits.loc[df_splits.index.isin(test_bal.index), \"split\"] = \"test_strict_balanced\"\n",
    "\n",
    "    # Report\n",
    "    report = {\n",
    "        \"n_train\": int((df_splits.split==\"train\").sum()),\n",
    "        \"n_val\": int((df_splits.split==\"val\").sum()),\n",
    "        \"n_test_strict_balanced\": int((df_splits.split==\"test_strict_balanced\").sum()),\n",
    "        \"n_test_patients\": len(test_pids),\n",
    "        \"n_val_patients\": len(val_pids),\n",
    "        \"strict_target_k\": target_k,\n",
    "        \"val_target_k\": val_target,\n",
    "        \"shortfalls_test\": short1 if 'short1' in locals() else {},\n",
    "        \"shortfalls_val\": shortv if 'shortv' in locals() else {},\n",
    "    }\n",
    "    return df_splits, report\n",
    "\n",
    "def assign_split_natural(df_all: pd.DataFrame, test_fraction=0.2, abs_n_test=None, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # Patient-level sampling to approximate target size\n",
    "    patients = list(df_all.patient_id.unique())\n",
    "    rng.shuffle(patients)\n",
    "    df = df_all.copy()\n",
    "\n",
    "    if abs_n_test is not None:\n",
    "        target = abs_n_test\n",
    "    else:\n",
    "        target = int(round(test_fraction * len(df)))\n",
    "\n",
    "    test_pids = set()\n",
    "    n = 0\n",
    "    for pid in patients:\n",
    "        k = df[df.patient_id==pid].shape[0]\n",
    "        if n + k <= target or n==0:\n",
    "            test_pids.add(pid); n += k\n",
    "        if n >= target:\n",
    "            break\n",
    "\n",
    "    remain = df[~df.patient_id.isin(test_pids)].copy()\n",
    "    test_nat = df[df.patient_id.isin(test_pids)].copy()\n",
    "\n",
    "    # Split remain into train/val (simple patient split; you can also balance val like above if desired)\n",
    "    rem_pids = list(remain.patient_id.unique())\n",
    "    rng.shuffle(rem_pids)\n",
    "    n_val = max(1, int(0.1 * len(rem_pids)))\n",
    "    val_pids = set(rem_pids[:n_val])\n",
    "    val = remain[remain.patient_id.isin(val_pids)].copy()\n",
    "    train = remain[~remain.patient_id.isin(val_pids)].copy()\n",
    "\n",
    "    df_splits = df_all.copy()\n",
    "    df_splits[\"split\"] = \"train\"\n",
    "    df_splits.loc[df_splits.index.isin(val.index), \"split\"] = \"val\"\n",
    "    df_splits.loc[df_splits.index.isin(test_nat.index), \"split\"] = \"test_natural\"\n",
    "\n",
    "    report = {\n",
    "        \"n_train\": int((df_splits.split==\"train\").sum()),\n",
    "        \"n_val\": int((df_splits.split==\"val\").sum()),\n",
    "        \"n_test_natural\": int((df_splits.split==\"test_natural\").sum()),\n",
    "        \"n_test_patients\": len(test_pids),\n",
    "        \"n_val_patients\": len(val_pids),\n",
    "    }\n",
    "    return df_splits, report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9270ee16",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Build the splits\n",
    "This will create:\n",
    "- `splits_strict_balanced.csv` (train/val/test_strict_balanced)\n",
    "- `splits_natural.csv` (train/val/test_natural)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b739af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Strict balanced protocol\n",
    "df_strict, rep_strict = assign_split_strict_balanced(ok, strict_test_k=STRICT_TEST_K, validation_k=VALIDATION_K, seed=RANDOM_SEED)\n",
    "strict_csv = OUT_DIR / \"splits_strict_balanced.csv\"\n",
    "df_strict.to_csv(strict_csv, index=False)\n",
    "print(\"Saved:\", strict_csv)\n",
    "print(json.dumps({k: (v if not isinstance(v, dict) else \"dict(size=\"+str(len(v))+\")\") for k,v in rep_strict.items()}, indent=2))\n",
    "\n",
    "# Natural protocol\n",
    "df_nat, rep_nat = assign_split_natural(ok, test_fraction=NATURAL_TEST_FRACTION, abs_n_test=ABS_N_TEST_IMAGES, seed=RANDOM_SEED)\n",
    "natural_csv = OUT_DIR / \"splits_natural.csv\"\n",
    "df_nat.to_csv(natural_csv, index=False)\n",
    "print(\"Saved:\", natural_csv)\n",
    "print(json.dumps(rep_nat, indent=2))\n",
    "\n",
    "display(df_strict['split'].value_counts())\n",
    "display(df_nat['split'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96643a",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Leakage & balance checks\n",
    "Verify **no patient overlap** across splits and inspect per-cell counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf477407",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_patient_leakage(df_splits: pd.DataFrame, split_col='split'):\n",
    "    problems = {}\n",
    "    splits = df_splits[split_col].unique().tolist()\n",
    "    pid_by_split = {s: set(df_splits[df_splits[split_col]==s]['patient_id'].unique()) for s in splits}\n",
    "    for i in range(len(splits)):\n",
    "        for j in range(i+1, len(splits)):\n",
    "            a, b = splits[i], splits[j]\n",
    "            inter = pid_by_split[a] & pid_by_split[b]\n",
    "            if inter:\n",
    "                problems[(a,b)] = inter\n",
    "    return problems\n",
    "\n",
    "def per_cell_table(df_splits, which_split):\n",
    "    g = df_splits[df_splits['split']==which_split].groupby(['magnification', 'subtype']).size().rename('count').reset_index()\n",
    "    return g.sort_values(['magnification','subtype'])\n",
    "\n",
    "# Strict\n",
    "leak_strict = check_patient_leakage(df_strict)\n",
    "print(\"Patient overlaps (strict):\", {str(k): len(v) for k,v in leak_strict.items()})\n",
    "print(\"StrictBalanced Test per-cell counts:\")\n",
    "display(per_cell_table(df_strict, 'test_strict_balanced'))\n",
    "\n",
    "# Natural\n",
    "leak_nat = check_patient_leakage(df_nat)\n",
    "print(\"Patient overlaps (natural):\", {str(k): len(v) for k,v in leak_nat.items()})\n",
    "print(\"Natural Test per-cell counts:\")\n",
    "display(per_cell_table(df_nat, 'test_natural'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6afcfd1",
   "metadata": {},
   "source": [
    "\n",
    "## 7. (Optional) Materialize images\n",
    "If `MATERIALIZE_IMAGES=True`, we create a folder structure like:\n",
    "```\n",
    "breakhis_ib_materialized/\n",
    "  strict_balanced/{train,val,test_strict_balanced}/<label>/<subtype>/<magnification>/...\n",
    "  natural/{train,val,test_natural}/<label>/<subtype>/<magnification>/...\n",
    "```\n",
    "Use `symlink` to save disk space, or `copy` if you need a fully independent tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5529ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def materialize_from_splits(df_splits: pd.DataFrame, root_out: Path, protocol_name: str, split_names: List[str], method='symlink'):\n",
    "    assert method in ('symlink','copy')\n",
    "    base = root_out / protocol_name\n",
    "    ensure_dir(base)\n",
    "    n=0\n",
    "    for split_name in split_names:\n",
    "        sub = base / split_name\n",
    "        ensure_dir(sub)\n",
    "        df_sub = df_splits[df_splits['split']==split_name].copy()\n",
    "        for _, row in df_sub.iterrows():\n",
    "            src = Path(row['path'])\n",
    "            # organize: <label>/<subtype>/<magnification>/filename\n",
    "            rel = Path(str(row['label'])) / str(row['subtype']) / f\"{int(row['magnification'])}X\"\n",
    "            dst_dir = sub / rel\n",
    "            ensure_dir(dst_dir)\n",
    "            dst = dst_dir / src.name\n",
    "            try:\n",
    "                if method == 'symlink':\n",
    "                    if dst.exists():\n",
    "                        continue\n",
    "                    os.symlink(src, dst)\n",
    "                else:\n",
    "                    if dst.exists():\n",
    "                        continue\n",
    "                    shutil.copy2(src, dst)\n",
    "                n += 1\n",
    "            except Exception as e:\n",
    "                print(\"Failed:\", src, \"->\", dst, \"|\", e)\n",
    "    print(f\"Materialized {n} files under {base}\")\n",
    "\n",
    "if MATERIALIZE_IMAGES:\n",
    "    # Strict\n",
    "    materialize_from_splits(df_strict, MATERIALIZE_DIR, protocol_name=\"strict_balanced\",\n",
    "                            split_names=[\"train\",\"val\",\"test_strict_balanced\"], method=MATERIALIZE_METHOD)\n",
    "    # Natural\n",
    "    materialize_from_splits(df_nat, MATERIALIZE_DIR, protocol_name=\"natural\",\n",
    "                            split_names=[\"train\",\"val\",\"test_natural\"], method=MATERIALIZE_METHOD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33273422",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Outputs\n",
    "- `metadata_raw.csv`: all parsed rows (including errors).\n",
    "- `metadata_ok.csv`: successfully parsed rows.\n",
    "- `parse_errors.csv`: rows where any of the critical fields were missing.\n",
    "- `splits_strict_balanced.csv`: patient-held-out, subtype-balanced-per-magnification (train/val/test_strict_balanced).\n",
    "- `splits_natural.csv`: patient-held-out, natural-frequency (train/val/test_natural).\n",
    "- `leakage & balance` printouts: confirm no patient overlap and check per-cell counts.\n",
    "- *(optional)* materialized folder trees with symlinks/copies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba9578",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Export a compact JSON report\n",
    "Handy for your IEEE DataPort page or README.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a66069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "report = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"n_images_total\": int(len(ok)),\n",
    "    \"n_patients_total\": int(ok['patient_id'].nunique()),\n",
    "    \"strict_balanced\": {\n",
    "        \"counts\": df_strict['split'].value_counts().to_dict(),\n",
    "        \"patients_per_split\": {s: int(ok[df_strict['split']==s]['patient_id'].nunique()) for s in df_strict['split'].unique()},\n",
    "    },\n",
    "    \"natural\": {\n",
    "        \"counts\": df_nat['split'].value_counts().to_dict(),\n",
    "        \"patients_per_split\": {s: int(ok[df_nat['split']==s]['patient_id'].nunique()) for s in df_nat['split'].unique()},\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(OUT_DIR / \"split_report.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(json.dumps(report, indent=2))\n",
    "print(\"Saved:\", OUT_DIR / \"split_report.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69146333",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Tips & troubleshooting\n",
    "- If `parse_errors.csv` is large, your local folder structure or filenames differ. Open that CSV to see which fields fail.  \n",
    "  You can extend `SUBTYPE_ALIASES`, tweak the regexes, or add custom rules.\n",
    "- If **StrictBalancedTest** couldn't meet the exact `K` in some cells, the code auto-adjusts `K` per magnification to the feasible minimum.  \n",
    "  You‚Äôll see this in the printed report ‚Äî that‚Äôs expected when some subtypes are scarce at a magnification.\n",
    "- To reproduce splits exactly, keep `RANDOM_SEED` fixed and commit the generated CSVs to your repo.\n",
    "- If you need **per-patient caps** (e.g., at most N tiles per patient per cell), you can add a cap before sampling in `sample_balanced_by_cell`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
